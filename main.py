# -*- coding: utf-8 -*-
"""main

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gl0MmQ6LDHgkPx2TFnEXcHb2eieujDSc
"""

@80535116321001 yeh tera project hai

# =========================================================
# üé™ CARNIVAL RISK ANALYTICS - OPTIMIZED LIGHTGBM PIPELINE
# =========================================================

# --- 1. IMPORT LIBRARIES ---
!pip install lightgbm scikit-learn --upgrade --quiet

import pandas as pd
import numpy as np
import lightgbm as lgb
import time
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
import seaborn as sns
import gc

print("‚úÖ Libraries imported successfully!")

# --- 2. LOAD DATA ---
train_df = pd.read_csv('train.csv')

# --- 3. FEATURE ENGINEERING ---
target = 'Premium Amount'
id_col = 'id'

# Convert date to datetime and extract features
train_df['Policy Start Date'] = pd.to_datetime(train_df['Policy Start Date'])
train_df['start_year'] = train_df['Policy Start Date'].dt.year
train_df['start_month'] = train_df['Policy Start Date'].dt.month
train_df['start_dayofweek'] = train_df['Policy Start Date'].dt.dayofweek
train_df['start_quarter'] = ((train_df['start_month']-1)//3 + 1)
train_df['is_weekend'] = (train_df['start_dayofweek'] >=5).astype(int)

# Drop original date column
X = train_df.drop(columns=[target, id_col, 'Policy Start Date'])
y = train_df[target]

# Drop rows with missing target
valid_y_indices = y.dropna().index
X = X.loc[valid_y_indices]
y = y.loc[valid_y_indices]

# Identify numerical and categorical columns
num_cols = X.select_dtypes(include=np.number).columns.tolist()
cat_cols = X.select_dtypes(include=['object']).columns.tolist()

print(f"\nNumerical Features: {len(num_cols)} | Categorical Features: {len(cat_cols)}")

# --- 4. PREPROCESSING ---
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median'))
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # Set sparse_output to False for easier DataFrame conversion
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, num_cols),
        ('cat', categorical_transformer, cat_cols)
    ],
    remainder='passthrough',
    n_jobs=-1
)

# --- 5. TRAIN/VALIDATION SPLIT ---
X_train, X_valid, y_train, y_valid = train_test_split(
    X, y, test_size=0.2, random_state=42
)
print("\n‚úÖ Data split into training and validation sets.")

# --- 6. PROCESS DATA AND CONVERT TO DATAFRAME ---
# Fit and transform training data
X_train_proc_dense = preprocessor.fit_transform(X_train)

# Get feature names after preprocessing
feature_names_out = preprocessor.get_feature_names_out()

# Convert to DataFrame
X_train_proc_df = pd.DataFrame(X_train_proc_dense, columns=feature_names_out, index=X_train.index)

# Transform validation data
X_valid_proc_dense = preprocessor.transform(X_valid)
X_valid_proc_df = pd.DataFrame(X_valid_proc_dense, columns=feature_names_out, index=X_valid.index)

# Explicitly convert categorical columns to 'category' dtype for LightGBM
# Identify which columns in the processed DataFrame are from the categorical transformer
processed_cat_cols = [col for col in feature_names_out if col.startswith('cat__')]
for col in processed_cat_cols:
    X_train_proc_df[col] = X_train_proc_df[col].astype('category')
    X_valid_proc_df[col] = X_valid_proc_df[col].astype('category')


print("\n‚úÖ Data processed and converted to DataFrames with categorical dtypes.")


# --- 7. LIGHTGBM MODEL ---
lgb_model = lgb.LGBMRegressor(
    objective='regression_l1',   # robust to outliers
    metric='rmse',
    n_estimators=2500,
    learning_rate=0.05,
    num_leaves=128,
    feature_fraction=0.95,
    bagging_fraction=0.95,
    bagging_freq=1,
    lambda_l1=0.3,
    lambda_l2=0.3,
    verbose=-1,
    n_jobs=-1,
    seed=42
)

# --- 8. TRAIN LIGHTGBM ---
print("\n‚ö° Training Optimized LightGBM Regressor...")
start_time = time.time()

# Train using the DataFrames
lgb_model.fit(
    X_train_proc_df, y_train,
    eval_set=[(X_valid_proc_df, y_valid)],
    eval_metric='rmse',
    callbacks=[lgb.early_stopping(100, verbose=False)]
)

lgb_train_time = time.time() - start_time
lgb_preds = lgb_model.predict(X_valid_proc_df)
lgb_rmse = np.sqrt(mean_squared_error(y_valid, lgb_preds))

print(f"\n‚úÖ Optimized LightGBM RMSE: {lgb_rmse:.4f}")
print(f"‚è±  Training Time: {lgb_train_time/60:.2f} minutes")

# --- 9. OPTIONAL: FEATURE IMPORTANCE ---
# Feature importance requires mapping back to original feature names which is complex with sparse output
# Skipping this for now or requires more sophisticated mapping

# --- 10. CLEANUP ---
gc.collect()
print("\nüéØ Optimized Pipeline Completed Successfully!")